{"cells":[{"cell_type":"markdown","id":"a00e032c","metadata":{"id":"hWgiQS0zkWJ5"},"source":["***Important*** DO NOT CLEAR THE OUTPUT OF THIS NOTEBOOK AFTER EXECUTION!!!"]},{"cell_type":"code","execution_count":54,"id":"5ac36d3a","metadata":{"id":"c0ccf76b","nbgrader":{"grade":false,"grade_id":"cell-Worker_Count","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"cf88b954-f39a-412a-d87e-660833e735b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["NAME         PLATFORM  PRIMARY_WORKER_COUNT  SECONDARY_WORKER_COUNT  STATUS   ZONE           SCHEDULED_DELETE\r\n","wikicluster  GCE       4                                             RUNNING  us-central1-a\r\n"]}],"source":["# if the following command generates an error, you probably didn't enable \n","# the cluster security option \"Allow API access to all Google Cloud services\"\n","# under Manage Security â†’ Project Access when setting up the cluster\n","!gcloud dataproc clusters list --region us-central1"]},{"cell_type":"markdown","id":"51cf86c5","metadata":{"id":"01ec9fd3"},"source":["# Imports & Setup"]},{"cell_type":"code","execution_count":55,"id":"bf199e6a","metadata":{"id":"32b3ec57","nbgrader":{"grade":false,"grade_id":"cell-Setup","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"fc0e315d-21e9-411d-d69c-5b97e4e5d629"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes"]},{"cell_type":"code","execution_count":56,"id":"d8f56ecd","metadata":{"id":"5609143b","nbgrader":{"grade":false,"grade_id":"cell-Imports","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"a24aa24b-aa75-4823-83ca-1d7deef0f0de"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","from nltk.stem.snowball import SnowballStemmer\n","import math\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":57,"id":"38a897f2","metadata":{"id":"b10cc999","nbgrader":{"grade":false,"grade_id":"cell-jar","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"8f93a7ec-71e0-49c1-fc81-9af385849a90"},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 247882 Mar  8 21:37 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"]}],"source":["# if nothing prints here you forgot to include the initialization script when starting the cluster\n","!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"code","execution_count":58,"id":"47900073","metadata":{"id":"d3f86f11","nbgrader":{"grade":false,"grade_id":"cell-pyspark-import","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *"]},{"cell_type":"code","execution_count":59,"id":"72bed56b","metadata":{"id":"5be6dc2a","nbgrader":{"grade":false,"grade_id":"cell-spark-version","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"07b4e22b-a252-42fb-fe46-d9050e4e7ca8","scrolled":true},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://wikicluster-m.c.stalwart-method-414418.internal:35199\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.3.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>yarn</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>PySparkShell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f32dcc89780>"]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"code","execution_count":60,"id":"980e62a5","metadata":{"id":"7adc1bf5","nbgrader":{"grade":false,"grade_id":"cell-bucket_name","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = 'wikibucket208' \n","full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    if b.name != 'graphframes.sh':\n","        paths.append(full_path+b.name)\n"]},{"cell_type":"code","execution_count":61,"id":"71ad8e70","metadata":{},"outputs":[],"source":["# Get some files from the bucket\n","def get_wiki_file_paths(all_wiki_files, num_files=3, all=False):\n","    # Dictionary to hold the file parts\n","    wiki_files_dict = {}\n","\n","    # Regular expression to extract file number and part\n","    file_pattern = re.compile(r\"multistream(\\d+)_?(part\\d+)?_preprocessed\\.parquet\")\n","    \n","    # Populate the dictionary with file numbers and their parts\n","    for file_path in all_wiki_files:\n","        match = file_pattern.search(file_path)\n","        if match:\n","            file_number = int(match.group(1))\n","            file_part = match.group(2) or \"part1\"  # Default to part1 if no part specified\n","\n","            if file_number not in wiki_files_dict:\n","                wiki_files_dict[file_number] = []\n","            wiki_files_dict[file_number].append((file_part, file_path))\n","\n","    # Sort the parts for each file number\n","    for file_number in wiki_files_dict:\n","        wiki_files_dict[file_number].sort()\n","    \n","    if(all):\n","        max_num = __builtin__.max(wiki_files_dict.keys())\n","        selected_files = list(sorted(wiki_files_dict.keys()))[:max_num]\n","    else:\n","        selected_files = list(sorted(wiki_files_dict.keys()))[:num_files]\n","\n","    # Collect paths, ensuring all parts of a file number are included\n","    wiki_files_paths = []\n","    for file_number in selected_files:\n","        for _, file_path in wiki_files_dict[file_number]:\n","            wiki_files_paths.append(file_path)\n","\n","    return wiki_files_paths\n"]},{"cell_type":"code","execution_count":62,"id":"d97cdb37","metadata":{},"outputs":[],"source":["NUM_BUCKETS = 150\n","bucket_name = 'wikibucket208'\n","base_dir = f'{bucket_name}'\n","\n","wiki_files_dir = f'{base_dir}/WikiFiles'\n","\n","index_filename = f'{bucket_name}/InvertedIndex/'\n","inverted_dir = f'{base_dir}/InvertedIndex'\n","posting_locs_dir = f'{inverted_dir}/posting_locs'\n","pl_body_dir = f'{posting_locs_dir}/body'\n","pl_title_dir = f'{posting_locs_dir}/title'"]},{"cell_type":"code","execution_count":63,"id":"5333d2af","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["all_files = True\n","num_wiki_files = 1   \n","\n","wiki_files_paths = get_wiki_file_paths(paths,num_wiki_files, all_files)\n","parquetFile = spark.read.parquet(*wiki_files_paths)"]},{"cell_type":"code","execution_count":64,"id":"14174dd1","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/plain":["6348910"]},"execution_count":64,"metadata":{},"output_type":"execute_result"}],"source":["# Count number of wiki pages\n","parquetFile.count()"]},{"cell_type":"markdown","id":"cac891c2","metadata":{"id":"13ZX4ervQkku"},"source":["***GCP setup is complete!*** If you got here without any errors you've earned 10 out of the 35 points of this part."]},{"cell_type":"markdown","id":"582c3f5e","metadata":{"id":"c0b0f215"},"source":["# Part 1 - Building an inverted index"]},{"cell_type":"markdown","id":"701811af","metadata":{"id":"gaaIoFViXyTg"},"source":["Let's import the inverted index module. Note that you need to use the staff-provided version called `inverted_index_gcp.py`, which contains helper functions to writing and reading the posting files similar to the Colab version, but with writing done to a Google Cloud Storage bucket."]},{"cell_type":"code","execution_count":65,"id":"121fe102","metadata":{"id":"04371c88","outputId":"327fe81b-80f4-4b3a-8894-e74720d92e35"},"outputs":[{"name":"stdout","output_type":"stream","text":["inverted_index_gcp.py\r\n"]}],"source":["# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n","%cd -q /home/dataproc\n","!ls inverted_index_gcp.py"]},{"cell_type":"code","execution_count":66,"id":"57c101a8","metadata":{"id":"2d3285d8","scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["24/03/11 19:11:49 WARN SparkContext: The path /home/dataproc/inverted_index_gcp.py has been added already. Overwriting of added paths is not supported in the current version.\n"]}],"source":["# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())"]},{"cell_type":"code","execution_count":67,"id":"c259c402","metadata":{"id":"2477a5b9"},"outputs":[],"source":["from inverted_index_gcp import InvertedIndex"]},{"cell_type":"markdown","id":"9de4dfc9","metadata":{},"source":["Indexing Functions"]},{"cell_type":"code","execution_count":68,"id":"f3ad8fea","metadata":{"id":"a4b6ee29","nbgrader":{"grade":false,"grade_id":"cell-token2bucket","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","\n","def token2bucket_id(token):\n","  return int(_hash(token),16) % NUM_BUCKETS\n","\n","# PLACE YOUR CODE HERE\n","# Word Count\n","# def word_count(text, id, stem_model='porter'):\n","#     tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","#     stemmed_tokens = stem_tokens(tokens, stem_model)\n","#     filtered_tokens = [token for token in stemmed_tokens if token not in all_stopwords]\n","    \n","#     tok_dic = Counter(filtered_tokens)\n","#     doc_length = len(filtered_tokens)  # Document length after stemming and removing stopwords\n","#     result = [(token, (id, count)) for token, count in tok_dic.items()]\n","#     return (id, (result, doc_length))\n","    \n","\n","def word_count(text, id):\n","\n","  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","\n","  # YOUR CODE HERE\n","  stemmed_tokens = stem_tokens(tokens)\n","\n","  tok_dic = {}\n","  non_stop_tokens = [token for token in tokens if token not in all_stopwords]\n","  for token in non_stop_tokens:\n","    tok_dic[token] = tok_dic.get(token, 0) + 1\n","  result = [(token, (id, tf)) for token, tf in tok_dic.items()]\n","  return result\n","    \n","    \n","# Reduce Word Count\n","def reduce_word_counts(unsorted_pl):\n","    return sorted(unsorted_pl, key=lambda x: x[0])\n","    \n","    \n","# Calculate DF\n","def calculate_df(postings):\n","  return postings.map(lambda x: (x[0], len(x[1])))\n","    \n","    \n","# Write Partition\n","def partition_postings_and_write(postings, save_dir):\n","  def write_bucket_to_disk(bucket_id_and_postings, save_dir):\n","        # Pass both the tuple and the bucket_name to write_a_posting_list\n","    return  InvertedIndex.write_a_posting_list(bucket_id_and_postings, save_dir, bucket_name)\n","\n","\n","  postings_with_bucket_id = postings.map(lambda x: (token2bucket_id(x[0]), (x[0], x[1])))\n","  postings_grouped_by_bucket = postings_with_bucket_id.groupByKey().mapValues(list)\n","  bucket_ids = postings_grouped_by_bucket.map(\n","        lambda bucket_postings: write_bucket_to_disk((bucket_postings[0], list(bucket_postings[1])), save_dir)\n","  )\n","  return bucket_ids"]},{"cell_type":"code","execution_count":69,"id":"010e111e","metadata":{},"outputs":[],"source":["# Tokenizer\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){,24}\"\"\", re.UNICODE)\n","def tokenize(text):\n","  return [token.group() for token in RE_WORD.finditer(text.lower())]\n","\n","# Stop Words function\n","nltk_stop_words = set(stopwords.words('english'))\n","\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","all_stopwords = nltk_stop_words.union(corpus_stopwords)\n","\n","def remove_stopwords(tokens):\n","    return [token for token in tokens if token not in all_stopwords]"]},{"cell_type":"code","execution_count":70,"id":"b27ba5cb","metadata":{},"outputs":[],"source":["# Stemmer Function\n","def stem_tokens(tokens, stem_model='porter'):\n","    if stem_model == 'porter':\n","        porter = PorterStemmer()\n","        stemmed_tokens = [porter.stem(token) for token in tokens]\n","    elif stem_model == 'snowball':\n","        snowball = SnowballStemmer(language='english')\n","        stemmed_tokens = [snowball.stem(token) for token in tokens]\n","    elif stem_model == None:\n","        return tokens\n","    else:\n","        raise ValueError(\"Unsupported stem model: {}\".format(stem_model))\n","    return stemmed_tokens\n","\n","\n","# Stemmer for Postinglists\n"]},{"cell_type":"code","execution_count":71,"id":"e3775723","metadata":{},"outputs":[],"source":["# Buckets so the data will be split efficiently\n","\n","from collections import defaultdict\n","def _hash(s):\n","  return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","def tokens2bucket_id(token):\n","  return int(_hash(token),16) % NUM_BUCKETS"]},{"cell_type":"markdown","id":"4ccfdcc4","metadata":{},"source":["<h2 style=\"color: blue;\">Doc Length Calculation</h3>\n"]},{"cell_type":"code","execution_count":72,"id":"0dde0cf1","metadata":{},"outputs":[],"source":["def doc_length_mapper(text, doc_id):\n","    tokens = tokenize(text)\n","    stemmed = stem_tokens(tokens)\n","    non_sw_tokens = remove_stopwords(stemmed)\n","    doc_len = len(non_sw_tokens)\n","    \n","    bucket_id = tokens2bucket_id(str(doc_id))\n","    \n","    return(bucket_id, (doc_id, doc_len))\n","\n","def doc_length_reducer(values):\n","    doc_lengths = {}\n","    total_length = 0\n","    doc_lengths = {doc_id: doc_len for doc_id, doc_len in values}\n","    total_length = __builtin__.sum(doc_lengths.values())\n","    doc_count = len(doc_lengths)\n","    avg_doc_len = total_length / doc_count if doc_count else 0\n","\n","    return doc_lengths, total_length, doc_count, avg_doc_len\n","\n","    \n","def write_to_gcp(save_dir, bucket_id, file_name, serialized_data):\n","    client = storage.Client()\n","    bucket = client.bucket(bucket_name)\n","    blob_name = f'{save_dir}/{bucket_id}{file_name}.pickle'\n","    blob = bucket.blob(blob_name)\n","    blob.upload_from_string(serialized_data)\n","\n","    "]},{"cell_type":"code","execution_count":73,"id":"4962c163","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["type = 'body'\n","doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd\n","\n","mapped_rdd = doc_text_pairs.map(lambda x: doc_length_mapper(x[0], x[1]))\n","grouped_rdd = mapped_rdd.groupByKey()\n","result_rdd = grouped_rdd.mapValues(doc_length_reducer)\n","results = result_rdd.collect()\n","\n","avgdl = 0\n","doc_num = 0\n","tot_doc_len = 0\n","\n","for bucket_id, (doc_lengths, total_length, doc_count, avg_doc_len) in results:\n","    doc_len_dir = f'InvertedIndex/doc_length/{type}'\n","    serialized_doc_lengths = pickle.dumps(doc_lengths)\n","    # Assuming you have a function to upload to cloud storage\n","    write_to_gcp(doc_len_dir, bucket_id, 'doc_len',serialized_doc_lengths)\n","    tot_doc_len += total_length\n","    doc_num += doc_count\n","\n","bm25_calc = {}\n","bm25_calc['avgdl'] = tot_doc_len / doc_num if doc_num else 0\n","bm25_calc['N'] = doc_num\n","serialized_title_calcs = pickle.dumps(bm25_calc)\n","body_bm25 = f'InvertedIndex/bm25'\n","write_to_gcp(body_bm25, '', f'{type}_bm25', serialized_title_calcs)\n","\n","\n","    # ------------------------------------------------------------------------------------------------------------    \n","\n","    \n","type = 'title'\n","doc_text_pairs = parquetFile.select(\"title\", \"id\").rdd\n","\n","mapped_rdd = doc_text_pairs.map(lambda x: doc_length_mapper(x[0], x[1]))\n","grouped_rdd = mapped_rdd.groupByKey()\n","result_rdd = grouped_rdd.mapValues(doc_length_reducer)\n","results = result_rdd.collect()\n","\n","avgdl = 0\n","doc_num = 0\n","tot_doc_len = 0\n","\n","for bucket_id, (doc_lengths, total_length, doc_count, avg_doc_len) in results:\n","    doc_len_dir = f'InvertedIndex/doc_length/{type}'\n","    serialized_doc_lengths = pickle.dumps(doc_lengths)\n","    # Assuming you have a function to upload to cloud storage\n","    write_to_gcp(doc_len_dir, bucket_id, 'doc_len',serialized_doc_lengths)\n","    tot_doc_len += total_length\n","    doc_num += doc_count\n","\n","bm25_calc = {}\n","bm25_calc['avgdl'] = tot_doc_len / doc_num if doc_num else 0\n","bm25_calc['N'] = doc_num\n","serialized_title_calcs = pickle.dumps(bm25_calc)\n","title_bm25 = f'InvertedIndex/bm25'\n","write_to_gcp(title_bm25, '', f'{type}_bm25', serialized_title_calcs)"]}],"metadata":{"celltoolbar":"Create Assignment","colab":{"collapsed_sections":[],"name":"assignment3_gcp.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}