{"cells":[{"cell_type":"markdown","id":"51cf86c5","metadata":{"id":"01ec9fd3"},"source":["# Imports & Setup"]},{"cell_type":"code","execution_count":13,"id":"bf199e6a","metadata":{"id":"32b3ec57","nbgrader":{"grade":false,"grade_id":"cell-Setup","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"fc0e315d-21e9-411d-d69c-5b97e4e5d629"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes"]},{"cell_type":"code","execution_count":14,"id":"d8f56ecd","metadata":{"id":"5609143b","nbgrader":{"grade":false,"grade_id":"cell-Imports","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"a24aa24b-aa75-4823-83ca-1d7deef0f0de"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","from nltk.stem.snowball import SnowballStemmer\n","import math\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":15,"id":"38a897f2","metadata":{"id":"b10cc999","nbgrader":{"grade":false,"grade_id":"cell-jar","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"8f93a7ec-71e0-49c1-fc81-9af385849a90"},"outputs":[{"name":"stdout","output_type":"stream","text":["-rw-r--r-- 1 root root 247882 Mar  8 21:37 /usr/lib/spark/jars/graphframes-0.8.2-spark3.1-s_2.12.jar\r\n"]}],"source":["# if nothing prints here you forgot to include the initialization script when starting the cluster\n","!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"code","execution_count":16,"id":"47900073","metadata":{"id":"d3f86f11","nbgrader":{"grade":false,"grade_id":"cell-pyspark-import","locked":true,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *"]},{"cell_type":"code","execution_count":17,"id":"57c101a8","metadata":{"id":"2d3285d8","scrolled":true},"outputs":[{"name":"stderr","output_type":"stream","text":["24/03/10 20:09:10 WARN SparkContext: The path /home/dataproc/inverted_index_gcp.py has been added already. Overwriting of added paths is not supported in the current version.\n"]}],"source":["# adding our python module to the cluster\n","sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())"]},{"cell_type":"code","execution_count":18,"id":"c259c402","metadata":{"id":"2477a5b9"},"outputs":[],"source":["from inverted_index_gcp import InvertedIndex"]},{"cell_type":"code","execution_count":19,"id":"72bed56b","metadata":{"id":"5be6dc2a","nbgrader":{"grade":false,"grade_id":"cell-spark-version","locked":true,"schema_version":3,"solution":false,"task":false},"outputId":"07b4e22b-a252-42fb-fe46-d9050e4e7ca8","scrolled":true},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - hive</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://wikicluster-m.c.stalwart-method-414418.internal:36929\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.3.2</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>yarn</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>PySparkShell</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f051cae9780>"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"code","execution_count":20,"id":"980e62a5","metadata":{"id":"7adc1bf5","nbgrader":{"grade":false,"grade_id":"cell-bucket_name","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["# Put your bucket name below and make sure you can access it without an error\n","bucket_name = 'wikibucket208' \n","full_path = f\"gs://{bucket_name}/\"\n","paths=[]\n","\n","client = storage.Client()\n","blobs = client.list_blobs(bucket_name)\n","for b in blobs:\n","    if b.name != 'graphframes.sh':\n","        paths.append(full_path+b.name)\n"]},{"cell_type":"code","execution_count":21,"id":"71ad8e70","metadata":{},"outputs":[],"source":["# Get some files from the bucket\n","def get_wiki_file_paths(all_wiki_files, num_files=3, all=False):\n","    # Dictionary to hold the file parts\n","    wiki_files_dict = {}\n","\n","    # Regular expression to extract file number and part\n","    file_pattern = re.compile(r\"multistream(\\d+)_?(part\\d+)?_preprocessed\\.parquet\")\n","    \n","    # Populate the dictionary with file numbers and their parts\n","    for file_path in all_wiki_files:\n","        match = file_pattern.search(file_path)\n","        if match:\n","            file_number = int(match.group(1))\n","            file_part = match.group(2) or \"part1\"  # Default to part1 if no part specified\n","\n","            if file_number not in wiki_files_dict:\n","                wiki_files_dict[file_number] = []\n","            wiki_files_dict[file_number].append((file_part, file_path))\n","\n","    # Sort the parts for each file number\n","    for file_number in wiki_files_dict:\n","        wiki_files_dict[file_number].sort()\n","    \n","    if(all):\n","        max_num = __builtin__.max(wiki_files_dict.keys())\n","        selected_files = list(sorted(wiki_files_dict.keys()))[:max_num]\n","    else:\n","        selected_files = list(sorted(wiki_files_dict.keys()))[:num_files]\n","\n","    # Collect paths, ensuring all parts of a file number are included\n","    wiki_files_paths = []\n","    for file_number in selected_files:\n","        for _, file_path in wiki_files_dict[file_number]:\n","            wiki_files_paths.append(file_path)\n","\n","    return wiki_files_paths\n"]},{"cell_type":"code","execution_count":22,"id":"d97cdb37","metadata":{},"outputs":[],"source":["NUM_BUCKETS = 150\n","bucket_name = 'wikibucket208'\n","base_dir = f'{bucket_name}'\n","\n","wiki_files_dir = f'{base_dir}/WikiFiles'\n","\n","index_filename = f'{bucket_name}/InvertedIndex/'\n","inverted_dir = f'{base_dir}/InvertedIndex'\n","posting_locs_dir = f'{inverted_dir}/posting_locs'\n","pl_body_dir = f'{posting_locs_dir}/body'\n","pl_title_dir = f'{posting_locs_dir}/title'"]},{"cell_type":"code","execution_count":23,"id":"5333d2af","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["all_files = False\n","num_wiki_files = 1   \n","\n","wiki_files_paths = get_wiki_file_paths(paths,num_wiki_files, all_files)\n","parquetFile = spark.read.parquet(*wiki_files_paths)"]},{"cell_type":"code","execution_count":24,"id":"14174dd1","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 5:=======================================>                   (2 + 1) / 3]\r"]},{"data":{"text/plain":["21084"]},"execution_count":24,"metadata":{},"output_type":"execute_result"},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["# Count number of wiki pages\n","parquetFile.count()"]},{"cell_type":"markdown","id":"582c3f5e","metadata":{"id":"c0b0f215"},"source":["# Building an inverted index"]},{"cell_type":"markdown","id":"9de4dfc9","metadata":{},"source":["Indexing Functions"]},{"cell_type":"code","execution_count":25,"id":"f3ad8fea","metadata":{"id":"a4b6ee29","nbgrader":{"grade":false,"grade_id":"cell-token2bucket","locked":false,"schema_version":3,"solution":true,"task":false}},"outputs":[],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","\n","all_stopwords = english_stopwords.union(corpus_stopwords)\n","\n","\n","def token2bucket_id(token):           # Hashing function\n","    return int(_hash(token),16) % NUM_BUCKETS\n","\n","\n","def word_count(text, id):             # Tokenizing text + TF\n","    tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","\n","    stemmed_tokens = stem_tokens(tokens)\n","\n","    tok_dic = {}\n","    non_stop_tokens = [token for token in tokens if token not in all_stopwords]\n","    for token in non_stop_tokens:\n","        tok_dic[token] = tok_dic.get(token, 0) + 1\n","    result = [(token, (id, tf)) for token, tf in tok_dic.items()]\n","    return result\n","   \n","    \n","def reduce_word_counts(unsorted_pl):  # Sorting posting list\n","    return sorted(unsorted_pl, key=lambda x: x[0])\n","    \n","    \n","def calculate_df(postings):           # Calculating DF\n","    return postings.map(lambda x: (x[0], len(x[1])))\n","    \n","    \n","def partition_postings_and_write(postings, save_dir):    # Writing posting locs to bucket\n","  def write_bucket_to_disk(bucket_id_and_postings, save_dir):\n","        # Pass both the tuple and the bucket_name to write_a_posting_list\n","    return  InvertedIndex.write_a_posting_list(bucket_id_and_postings, save_dir, bucket_name)\n","\n","  postings_with_bucket_id = postings.map(lambda x: (token2bucket_id(x[0]), (x[0], x[1])))\n","  postings_grouped_by_bucket = postings_with_bucket_id.groupByKey().mapValues(list)\n","  bucket_ids = postings_grouped_by_bucket.map(\n","        lambda bucket_postings: write_bucket_to_disk((bucket_postings[0], list(bucket_postings[1])), save_dir)\n","  )\n","  return bucket_ids"]},{"cell_type":"code","execution_count":26,"id":"010e111e","metadata":{},"outputs":[],"source":["# Tokenizer\n","RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){,24}\"\"\", re.UNICODE)\n","def tokenize(text):\n","  return [token.group() for token in RE_WORD.finditer(text.lower())]\n","\n","# Stop Words function\n","nltk_stop_words = set(stopwords.words('english'))\n","\n","corpus_stopwords = [\"category\", \"references\", \"also\", \"external\", \"links\", \n","                    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\", \n","                    \"part\", \"thumb\", \"including\", \"second\", \"following\", \n","                    \"many\", \"however\", \"would\", \"became\"]\n","all_stopwords = nltk_stop_words.union(corpus_stopwords)\n","\n","def remove_stopwords(tokens):\n","    return [token for token in tokens if token not in all_stopwords]"]},{"cell_type":"code","execution_count":27,"id":"b27ba5cb","metadata":{},"outputs":[],"source":["# Stemmer Function\n","def stem_tokens(tokens, stem_model='porter'):\n","    if stem_model == 'porter':\n","        porter = PorterStemmer()\n","        stemmed_tokens = [porter.stem(token) for token in tokens]\n","    elif stem_model == 'snowball':\n","        snowball = SnowballStemmer(language='english')\n","        stemmed_tokens = [snowball.stem(token) for token in tokens]\n","    elif stem_model == None:\n","        return tokens\n","    else:\n","        raise ValueError(\"Unsupported stem model: {}\".format(stem_model))\n","    return stemmed_tokens"]},{"cell_type":"code","execution_count":28,"id":"e3775723","metadata":{},"outputs":[],"source":["# Hashing data to different buckets so the data will be split efficiently\n","from collections import defaultdict\n","def _hash(s):\n","  return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","def tokens2bucket_id(token):\n","  return int(_hash(token),16) % NUM_BUCKETS"]},{"cell_type":"markdown","id":"fc38ce00","metadata":{},"source":["<h2 style=\"color: blue;\">Indexing Body Posting Locs</h3>\n"]},{"cell_type":"code","execution_count":29,"id":"285078d0","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["    # Save Posting_locs Body\n","save_dir = 'InvertedIndex/posting_locs/body/'\n","doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd\n","\n","    # word counts map\n","word_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","\n","    # filtering postings and calculate df\n","postings_filtered = postings.filter(lambda x: len(x[1])>50)\n","w2df = calculate_df(postings_filtered)\n","body_w2df_dict = w2df.collectAsMap()\n","\n","    # partition posting lists and write out\n","_ = partition_postings_and_write(postings_filtered, save_dir).collect()"]},{"cell_type":"markdown","id":"f9185c3d","metadata":{},"source":["<h2 style=\"color: blue;\">Indexing Body</h3>\n"]},{"cell_type":"code","execution_count":30,"id":"ab3296f4","metadata":{"id":"Opl6eRNLM5Xv","nbgrader":{"grade":true,"grade_id":"collect-posting","locked":true,"points":0,"schema_version":3,"solution":false,"task":false}},"outputs":[],"source":["\n","save_dir = 'InvertedIndex'\n","\n","    # Write Index values (tf/df) for Body\n","body_super_posting_locs = defaultdict(list)\n","body_posting_locs_prefix = 'InvertedIndex/posting_locs/body/'\n","\n","for blob in client.list_blobs(bucket_name, prefix=body_posting_locs_prefix):\n","    if not blob.name.endswith(\"pickle\"):\n","        continue\n","    with blob.open(\"rb\") as f:\n","        posting_locs = pickle.load(f)\n","        for k, v in posting_locs.items():\n","            body_super_posting_locs[k].extend(v)\n","\n","\n","    # Create inverted index instance\n","inverted = InvertedIndex()\n","    # Adding the posting locations dictionary to the inverted index\n","inverted.posting_locs = body_super_posting_locs\n","    # Add the token - df dictionary to the inverted index\n","inverted.df = body_w2df_dict\n","    # write the global stats out\n","inverted.write_index(save_dir, 'body_index', bucket_name)"]}],"metadata":{"celltoolbar":"Create Assignment","colab":{"collapsed_sections":[],"name":"assignment3_gcp.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"}},"nbformat":4,"nbformat_minor":5}
